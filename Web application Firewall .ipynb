{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a905ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import urllib.parse\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# List of bad words to check in the URL path\n",
    "badwords = ['sleep', 'uid', 'select', 'waitfor', 'delay', 'system', 'union', 'order by', 'group by', 'admin', 'drop', 'script']\n",
    "\n",
    "# Function to extract features from the URL path and body\n",
    "def ExtractFeatures(path, body):\n",
    "    path = str(path)\n",
    "    body = str(body)\n",
    "    combined_raw = path + body\n",
    "    raw_percentages = combined_raw.count(\"%\")\n",
    "    raw_spaces = combined_raw.count(\" \")\n",
    "\n",
    "    # Check if both counts exceed the threshold\n",
    "    raw_percentages_count = raw_percentages if raw_percentages > 3 else 0\n",
    "    raw_spaces_count = raw_spaces if raw_spaces > 3 else 0\n",
    "\n",
    "    # Decode the path and body for other feature extractions\n",
    "    path_decoded = urllib.parse.unquote_plus(path)\n",
    "    body_decoded = urllib.parse.unquote_plus(body)\n",
    "\n",
    "    single_q = path_decoded.count(\"'\") + body_decoded.count(\"'\")\n",
    "    double_q = path_decoded.count(\"\\\"\") + body_decoded.count(\"\\\"\")\n",
    "    dashes = path_decoded.count(\"--\") + body_decoded.count(\"--\")\n",
    "    braces = path_decoded.count(\"(\") + body_decoded.count(\"(\")\n",
    "    spaces = path_decoded.count(\" \") + body_decoded.count(\" \")\n",
    "    semicolons = path_decoded.count(\";\") + body_decoded.count(\";\")\n",
    "    angle_brackets = path_decoded.count(\"<\") + path_decoded.count(\">\") + body_decoded.count(\"<\") + body_decoded.count(\">\")\n",
    "    special_chars = sum(path_decoded.count(c) + body_decoded.count(c) for c in '$&|')\n",
    "\n",
    "    badwords_count = sum(path_decoded.lower().count(word) + body_decoded.lower().count(word) for word in badwords)\n",
    "\n",
    "    path_length = len(path_decoded)\n",
    "    body_length = len(body_decoded)\n",
    "\n",
    "    return [single_q, double_q, dashes, braces, spaces, raw_percentages_count, semicolons, angle_brackets, special_chars, path_length, body_length, badwords_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c5dfedb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method              0\n",
      "path                0\n",
      "body              422\n",
      "single_q            0\n",
      "double_q            0\n",
      "dashes              0\n",
      "braces              0\n",
      "spaces              0\n",
      "percentages         0\n",
      "semicolons          0\n",
      "angle_brackets      0\n",
      "special_chars       0\n",
      "path_length         0\n",
      "body_length         0\n",
      "badwords_count      0\n",
      "class               0\n",
      "dtype: int64\n",
      "Accuracy: 0.97\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97        57\n",
      "           1       1.00      0.93      0.96        43\n",
      "\n",
      "    accuracy                           0.97       100\n",
      "   macro avg       0.97      0.97      0.97       100\n",
      "weighted avg       0.97      0.97      0.97       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import urllib.parse\n",
    "import pickle\n",
    "\n",
    "# Load dataset\n",
    "http = pd.read_csv(r'C:\\Users\\PRATHAM\\Documents\\python project\\All_data.csv')\n",
    "missing_values = http.isna().sum()\n",
    "print(missing_values)\n",
    "\n",
    "# Check if the necessary columns exist\n",
    "required_columns = ['path', 'body', 'class']\n",
    "missing_columns = [col for col in required_columns if col not in http.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"Error: The dataset is missing the following columns: {', '.join(missing_columns)}\")\n",
    "\n",
    "# Handle missing values (fill with mean for numeric columns as an example)\n",
    "# Note: Fill numeric columns only, if necessary\n",
    "#http.fillna(http.mean(), inplace=True)\n",
    "\n",
    "# Dummy badwords list for the example; replace it with actual bad words list\n",
    "badwords = ['badword1', 'badword2']\n",
    "\n",
    "\n",
    "# Extract features from the 'path' and 'body' columns\n",
    "http['features'] = http.apply(lambda row: ExtractFeatures(row['path'], row['body']), axis=1)\n",
    "\n",
    "# Prepare the feature matrix and the labels\n",
    "X = np.array(http['features'].tolist())\n",
    "y = http['class'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a logistic regression model\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "filename = 'training_model.pkl'\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962b2c03-d63e-4e34-ad9f-16fc82b40610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening on http://127.0.0.1:8080\n",
      "['http:', '', 'detectportal.firefox.com', 'canonical.html']\n",
      "0\n",
      "['http:', '', 'detectportal.firefox.com', 'canonical.html']\n",
      "0\n",
      "['http:', '', 'detectportal.firefox.com', 'canonical.html']\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [27/May/2024 11:51:00] code 501, message Unsupported method ('CONNECT')\n",
      "127.0.0.1 - - [27/May/2024 11:51:00] \"CONNECT sb-ssl.google.com:443 HTTP/1.1\" 501 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http:', '', 'detectportal.firefox.com', 'canonical.html']\n",
      "0\n",
      "['http:', '', 'detectportal.firefox.com', 'canonical.html']\n",
      "0\n",
      "['http:', '', 'demo.testfire.net', 'search.jsp?query=1234+%27+AND+1%3D0+UNION+ALL+SELECT+%27admin%27%2C+%2781dc9bdb52d04dc20036dbd8313ed055']\n",
      "1\n",
      "Intrusion Detected\n",
      "['http:', '', 'detectportal.firefox.com', 'canonical.html']\n",
      "0\n",
      "['http:', '', 'detectportal.firefox.com', 'canonical.html']\n",
      "0\n",
      "['http:', '', 'detectportal.firefox.com', 'canonical.html']\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [27/May/2024 11:51:16] code 501, message Unsupported method ('CONNECT')\n",
      "127.0.0.1 - - [27/May/2024 11:51:16] \"CONNECT services.addons.mozilla.org:443 HTTP/1.1\" 501 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http:', '', 'detectportal.firefox.com', 'canonical.html']\n",
      "0\n",
      "['http:', '', 'detectportal.firefox.com', 'canonical.html']\n",
      "0\n",
      "['http:', '', 'detectportal.firefox.com', 'canonical.html']\n",
      "0\n",
      "['http:', '', 'detectportal.firefox.com', 'canonical.html']\n",
      "0\n",
      "\n",
      "Keyboard interrupt received, exiting.\n"
     ]
    }
   ],
   "source": [
    "import http.server\n",
    "import socketserver\n",
    "import urllib.request\n",
    "from urllib.parse import urlparse\n",
    "import joblib\n",
    "\n",
    "# Load the ML model\n",
    "model = joblib.load(\"ml_model.pkl\")\n",
    "print(\"‚úÖ Model loaded successfully.\")\n",
    "\n",
    "PORT = 8080\n",
    "\n",
    "class ProxyHTTPRequestHandler(http.server.SimpleHTTPRequestHandler):\n",
    "\n",
    "    def do_GET(self):\n",
    "        if self.path == '/':\n",
    "            # Serve homepage\n",
    "            self.send_response(200)\n",
    "            self.send_header('Content-type', 'text/html')\n",
    "            self.end_headers()\n",
    "            self.wfile.write(b\"<h1>Welcome to Web Application Firewall Proxy</h1>\")\n",
    "        elif self.path.startswith('/proxy_route/'):\n",
    "            # Extract domain\n",
    "            target_domain = self.path[len('/proxy_route/'):]\n",
    "            url = f\"http://{target_domain}\"\n",
    "\n",
    "            print(f\"üîç Scanning URL: {url}\")\n",
    "\n",
    "            # Use ML model to predict (you can expand this to actually process input)\n",
    "            prediction = model.predict([[1, 2, 3, 4]])  # Replace with actual features\n",
    "\n",
    "            if prediction[0] == 1:\n",
    "                self.send_response(403)\n",
    "                self.end_headers()\n",
    "                self.wfile.write(b\"<h1>üö´ Access Blocked: Malicious URL</h1>\")\n",
    "                return\n",
    "\n",
    "            try:\n",
    "                # Add User-Agent header to avoid 403 Forbidden\n",
    "                req = urllib.request.Request(url)\n",
    "                req.add_header(\"User-Agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/122.0.0.0 Safari/537.36\")\n",
    "\n",
    "                with urllib.request.urlopen(req) as response:\n",
    "                    content = response.read()\n",
    "                    self.send_response(200)\n",
    "                    self.send_header('Content-type', response.headers.get_content_type())\n",
    "                    self.end_headers()\n",
    "                    self.wfile.write(content)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Proxy error: {e}\")\n",
    "                self.send_response(500)\n",
    "                self.end_headers()\n",
    "                self.wfile.write(b\"<h1>‚ùå Proxy Error: Unable to fetch the requested page.</h1>\")\n",
    "        else:\n",
    "            self.send_error(404, \"Page Not Found\")\n",
    "\n",
    "    def do_POST(self):\n",
    "        if self.path.startswith('/proxy_route/'):\n",
    "            content_length = int(self.headers['Content-Length'])\n",
    "            post_data = self.rfile.read(content_length)\n",
    "\n",
    "            target_domain = self.path[len('/proxy_route/'):]\n",
    "            url = f\"http://{target_domain}\"\n",
    "\n",
    "            print(f\"üîç Scanning URL (POST): {url}\")\n",
    "\n",
    "            prediction = model.predict([[1, 2, 3, 4]])  # Replace with real data\n",
    "\n",
    "            if prediction[0] == 1:\n",
    "                self.send_response(403)\n",
    "                self.end_headers()\n",
    "                self.wfile.write(b\"<h1>üö´ Access Blocked: Malicious POST Request</h1>\")\n",
    "                return\n",
    "\n",
    "            try:\n",
    "                req = urllib.request.Request(url, data=post_data, method='POST')\n",
    "                req.add_header('User-Agent', 'Mozilla/5.0')\n",
    "                req.add_header('Content-Type', 'application/x-www-form-urlencoded')\n",
    "\n",
    "                with urllib.request.urlopen(req) as response:\n",
    "                    content = response.read()\n",
    "                    self.send_response(200)\n",
    "                    self.send_header('Content-type', response.headers.get_content_type())\n",
    "                    self.end_headers()\n",
    "                    self.wfile.write(content)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Proxy error: {e}\")\n",
    "                self.send_response(500)\n",
    "                self.end_headers()\n",
    "                self.wfile.write(b\"<h1>‚ùå Proxy Error: Unable to process POST request.</h1>\")\n",
    "        else:\n",
    "            self.send_error(404, \"POST Route Not Found\")\n",
    "\n",
    "# Start server\n",
    "with socketserver.TCPServer((\"\", PORT), ProxyHTTPRequestHandler) as httpd:\n",
    "    print(f\"üöÄ Listening on http://127.0.0.1:{PORT}\")\n",
    "    httpd.serve_forever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d07922af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to testing_datas_with_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "testing_data = pd.read_csv(r'P:\\WAF\\Testing_data.csv')\n",
    "if 'path' in testing_data.columns and 'body' in testing_data.columns:\n",
    "        # Extract features from the testing data paths and bodies\n",
    "    test_features = testing_data.apply(lambda row: ExtractFeatures(row['path'], row['body']), axis=1).tolist()\n",
    "\n",
    "        # Convert test_features to a 2D numpy array\n",
    "    test_features = np.array(test_features)\n",
    "\n",
    "        # Predict whether each data point is good or bad\n",
    "    predictions = model.predict(test_features)\n",
    "\n",
    "        # Add the predictions to the testing data DataFrame\n",
    "    testing_data['Prediction'] = predictions\n",
    "\n",
    "        # Save the testing data with predictions to a new CSV file\n",
    "    testing_data.to_csv(r'P:\\WAF\\Testing_result.csv', index=False)\n",
    "    print(\"Predictions saved to testing_datas_with_predictions.csv\")\n",
    "else:\n",
    "    print(\"Error: The testing data must contain 'path' and 'body'¬†columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacaa8a5-889f-4f34-b414-a03ce5750498",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
